{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8B15Ncbjc1N2"
      },
      "source": [
        "#Mount your google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rt4ktqMHWRry",
        "outputId": "fc1ae470-bd54-4e7e-8cdc-79e27399ef0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycjABgQYc6AB"
      },
      "source": [
        "#Install pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uhp9c_MWR8Ez"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision\n",
        "!pip install -q cityscapesscripts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD2v9XmZtm5g"
      },
      "source": [
        "#Define ICNET model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_ctYVtXtyjS"
      },
      "source": [
        "## Model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uOEvJt0iSDIv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "class PyramidPooling(nn.Module):\n",
        "    def __init__(self, in_channels, pool_sizes=[1, 2, 3, 6]):\n",
        "        super(PyramidPooling, self).__init__()\n",
        "        self.pools = nn.ModuleList([\n",
        "            nn.AdaptiveAvgPool2d(output_size=size) for size in pool_sizes\n",
        "        ])\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels, in_channels // len(pool_sizes), kernel_size=1, bias=False) for _ in pool_sizes\n",
        "        ])\n",
        "        out_channels = in_channels + (in_channels // len(pool_sizes)) * len(pool_sizes)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        size = x.shape[2:]\n",
        "        out = [x]\n",
        "        for pool, conv in zip(self.pools, self.convs):\n",
        "            pooled = pool(x)\n",
        "            conv_out = conv(pooled)\n",
        "            out.append(F.interpolate(conv_out, size=size, mode='bilinear', align_corners=True))\n",
        "        out = torch.cat(out, dim=1)\n",
        "        return self.relu(self.bn(out))\n",
        "\n",
        "\n",
        "class CascadeFeatureFusion(nn.Module):\n",
        "    \"\"\"CFF Unit\"\"\"\n",
        "    def __init__(self, low_channels, high_channels, out_channels, n_classes):\n",
        "        super(CascadeFeatureFusion, self).__init__()\n",
        "        self.conv_low = nn.Sequential(\n",
        "            nn.Conv2d(low_channels, out_channels, kernel_size=3, padding=2, dilation=2, bias=False),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        )\n",
        "        self.conv_high = nn.Sequential(\n",
        "            nn.Conv2d(high_channels, out_channels, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        )\n",
        "        self.conv_low_cls = nn.Conv2d(out_channels, n_classes, kernel_size=1, bias=False)\n",
        "\n",
        "    def forward(self, x_low, x_high):\n",
        "        x_low = F.interpolate(x_low, size=x_high.shape[2:], mode='bilinear', align_corners=True)\n",
        "        x_low = self.conv_low(x_low)\n",
        "        x_high = self.conv_high(x_high)\n",
        "        x = F.relu(x_low + x_high, inplace=True)\n",
        "        x_low_cls = self.conv_low_cls(x_low)\n",
        "        return x, x_low_cls\n",
        "\n",
        "\n",
        "class SemanticSegmentationModel(nn.Module):\n",
        "    def __init__(self, n_classes=8):\n",
        "        super(SemanticSegmentationModel, self).__init__()\n",
        "        resnet = resnet50(weights='IMAGENET1K_V1')\n",
        "        self.initial_layers = nn.Sequential(\n",
        "            resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool\n",
        "        )\n",
        "        self.layer1 = resnet.layer1\n",
        "        self.layer2 = resnet.layer2\n",
        "        self.layer3 = resnet.layer3\n",
        "        self.layer4 = resnet.layer4\n",
        "\n",
        "        self.pyramid_pooling = PyramidPooling(2048)\n",
        "\n",
        "        self.cff_24 = CascadeFeatureFusion(low_channels=4096, high_channels=1024, out_channels=512, n_classes=n_classes)\n",
        "        self.cff_12 = CascadeFeatureFusion(low_channels=512, high_channels=512, out_channels=128, n_classes=n_classes)\n",
        "\n",
        "        self.classifier = nn.Conv2d(128, n_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.initial_layers(x)\n",
        "        c1 = self.layer1(x)\n",
        "        c2 = self.layer2(c1)\n",
        "        c3 = self.layer3(c2)\n",
        "        c4 = self.layer4(c3)\n",
        "\n",
        "        # Pass through PyramidPooling\n",
        "        c4 = self.pyramid_pooling(c4)\n",
        "\n",
        "        # Feature fusion\n",
        "        c3_c4, aux_c3_c4 = self.cff_24(c4, c3)  # c3: 1024 channels, c4: 4096 channels\n",
        "        c1_c2_c3, aux_c1_c2 = self.cff_12(c3_c4, c2)  # c3_c4: 512 channels, c2: 512 channels\n",
        "\n",
        "        out = self.classifier(c1_c2_c3)\n",
        "\n",
        "        # Upsample to match original input size\n",
        "        out = F.interpolate(out, size=(128, 256), mode='bilinear', align_corners=False)\n",
        "        aux_c3_c4 = F.interpolate(aux_c3_c4, size=(128, 256), mode='bilinear', align_corners=False)\n",
        "        aux_c1_c2 = F.interpolate(aux_c1_c2, size=(128, 256), mode='bilinear', align_corners=False)\n",
        "        return out, aux_c3_c4, aux_c1_c2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKSWA6pj1QLi"
      },
      "source": [
        "#Fetch data and transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7DPAc2y7SF_W"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "class CityscapesCustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, dataset_type=\"clean\", transform=None):\n",
        "        \"\"\"\n",
        "        Custom Cityscapes Dataset.\n",
        "        - root_dir: Root directory of the dataset containing images and labels.\n",
        "        - dataset_type: Either \"clean\" or \"adversarial\".\n",
        "        - transform: Transformations to apply to images (labels remain untransformed).\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Define subdirectories for images and labels\n",
        "        if dataset_type == \"clean\":\n",
        "            self.image_dir = os.path.join(root_dir, \"Clean Images\")\n",
        "        elif dataset_type == \"adversarial\":\n",
        "            self.image_dir = os.path.join(root_dir, \"Adverserial Images\")\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown dataset_type: {dataset_type}. Use 'clean' or 'adversarial'.\")\n",
        "\n",
        "        self.label_dir = os.path.join(root_dir, \"Labels\")\n",
        "\n",
        "        # Get list of all images and corresponding labels\n",
        "        self.image_paths = sorted(os.listdir(self.image_dir))\n",
        "        self.label_paths = sorted(os.listdir(self.label_dir))\n",
        "\n",
        "        # Ensure correspondence between images and labels\n",
        "        assert len(self.image_paths) == len(self.label_paths), \"Mismatch between images and labels\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load the image and label\n",
        "        image_path = os.path.join(self.image_dir, self.image_paths[idx])\n",
        "        label_path = os.path.join(self.label_dir, self.label_paths[idx])\n",
        "\n",
        "        # Load image and label\n",
        "        image = Image.open(image_path).convert(\"RGB\")  # Load as RGB\n",
        "        label = Image.open(label_path)  # Labels are grayscale\n",
        "\n",
        "        # Apply transformation to the image only\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Convert label to a tensor without transformation\n",
        "        label = torch.tensor(np.array(label), dtype=torch.long)\n",
        "\n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocd4ZrtUkis6"
      },
      "source": [
        "#Make some transformations to the images and load them to a dataloader for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "II1j_JqpXkcN"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import random\n",
        "from torch.utils.data import ConcatDataset, random_split, Subset\n",
        "\n",
        "# Define transformations\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "# Root directory containing the dataset folders\n",
        "root_dir = '/content/drive/MyDrive/Cityscapes/Final_Dataset'\n",
        "\n",
        "clean_dataset = CityscapesCustomDataset(root_dir=root_dir, dataset_type=\"clean\", transform=image_transform)\n",
        "adv_dataset = CityscapesCustomDataset(root_dir=root_dir, dataset_type=\"adversarial\", transform=image_transform)\n",
        "\n",
        "# Combine datasets\n",
        "combined_dataset = ConcatDataset([clean_dataset, adv_dataset])\n",
        "\n",
        "# Get indices of adversarial images in the combined dataset\n",
        "adv_indices = list(range(len(clean_dataset), len(combined_dataset)))\n",
        "\n",
        "# Shuffle the adversarial indices\n",
        "np.random.shuffle(adv_indices)\n",
        "\n",
        "# Define the validation size\n",
        "val_size = int(0.2 * len(combined_dataset))\n",
        "\n",
        "# Ensure val dataset contains only adversarial images\n",
        "val_indices = adv_indices[:val_size]\n",
        "\n",
        "# Remaining indices for training\n",
        "train_indices = list(set(range(len(combined_dataset))) - set(val_indices))\n",
        "\n",
        "# Create Subsets for training and validation datasets\n",
        "train_dataset = Subset(combined_dataset, train_indices)\n",
        "val_dataset = Subset(combined_dataset, val_indices)\n",
        "\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=8, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True, num_workers=8, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9XikfJxkp5z"
      },
      "source": [
        "#Define progress bar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIU2FHmbcfZ1"
      },
      "outputs": [],
      "source": [
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-1U2IMmkwYw"
      },
      "source": [
        "#Define function to calculate the MIoU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rRfnndkodVXC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def calculate_iou(pred, target, num_classes):\n",
        "    \"\"\"\n",
        "    Calculate IoU for each class over a batch.\n",
        "    Args:\n",
        "        pred: Predicted tensor of shape (batch_size, height, width)\n",
        "        target: Ground truth tensor of shape (batch_size, height, width)\n",
        "        num_classes: Total number of classes.\n",
        "    Returns:\n",
        "        miou: Mean IoU over all classes.\n",
        "    \"\"\"\n",
        "    ious = []\n",
        "\n",
        "    # Flatten predictions and targets across the batch\n",
        "    pred = pred.view(-1)  # Shape: (batch_size * height * width)\n",
        "    target = target.view(-1)  # Shape: (batch_size * height * width)\n",
        "\n",
        "    for cls in range(num_classes):\n",
        "        # True Positive (intersection)\n",
        "        intersection = ((pred == cls) & (target == cls)).sum().item()\n",
        "        # Union\n",
        "        union = ((pred == cls) | (target == cls)).sum().item()\n",
        "\n",
        "        if union == 0:\n",
        "            ious.append(float('nan'))  # Ignore classes not present in the batch\n",
        "        else:\n",
        "            ious.append(intersection / union)\n",
        "\n",
        "    # Calculate mean IoU, ignoring NaN values\n",
        "    miou = np.nanmean(ious)\n",
        "    return miou"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpRLHCtck0ac"
      },
      "source": [
        "#Define a train and validate function for the model to learn form the images in the dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4bapg0UtXlQe"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, criterion, optimizer, device, num_classes=8):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with tqdm(dataloader, desc=\"Training\", leave=False) as pbar:\n",
        "        for imgs, labels in pbar:\n",
        "            # Move data to the device\n",
        "            imgs, labels = imgs.to(device), labels.to(device, dtype=torch.long)\n",
        "\n",
        "            # Mask out ignored regions (-1)\n",
        "            valid_mask = labels >= 0  # Boolean mask for valid pixels\n",
        "            labels = torch.where(valid_mask, labels, torch.zeros_like(labels))  # Replace -1 with 0 for loss computation\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(imgs)\n",
        "\n",
        "            # Handle different output formats (e.g., tuple, dictionary, tensor)\n",
        "            if isinstance(outputs, dict) and \"out\" in outputs:\n",
        "                outputs = outputs[\"out\"]  # Extract primary output (e.g., from torchvision models)\n",
        "            elif isinstance(outputs, tuple):\n",
        "                outputs = outputs[0]  # Extract the first element if outputs is a tuple\n",
        "\n",
        "            # Resize outputs to match labels\n",
        "            if outputs.shape[-2:] != labels.shape[-2:]:\n",
        "                outputs = F.interpolate(outputs, size=labels.shape[-2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "            # Compute loss only on valid pixels\n",
        "            loss = criterion(outputs, labels) * valid_mask.float()\n",
        "            loss = loss.sum() / valid_mask.sum()  # Normalize by the number of valid pixels\n",
        "\n",
        "            # Backward pass and optimizer step\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track and log loss\n",
        "            total_loss += loss.item()\n",
        "            pbar.set_postfix({'Avg Loss': total_loss / (pbar.n + 1)})\n",
        "\n",
        "    return total_loss / len(dataloader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5snsxDmV9gN"
      },
      "source": [
        "Create a validation function to validate the model on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qSINttjwWATh"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "def validate(model, dataloader, criterion, device, num_classes=8):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_miou = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        with tqdm(dataloader, desc=\"Validating\", leave=False) as pbar:\n",
        "            for imgs, labels in pbar:\n",
        "                # Move data to the device\n",
        "                imgs, labels = imgs.to(device), labels.to(device, dtype=torch.long)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(imgs)\n",
        "\n",
        "                # Handle different output formats (e.g., tuple, dictionary, tensor)\n",
        "                if isinstance(outputs, dict) and \"out\" in outputs:\n",
        "                    outputs = outputs[\"out\"]  # Extract primary output (e.g., from torchvision models)\n",
        "                elif isinstance(outputs, tuple):\n",
        "                    outputs = outputs[0]  # Extract the first element if outputs is a tuple\n",
        "\n",
        "                # Resize outputs to match labels\n",
        "                if outputs.shape[-2:] != labels.shape[-2:]:\n",
        "                    outputs = F.interpolate(outputs, size=labels.shape[-2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = criterion(outputs, labels)\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                # Calculate mIoU\n",
        "                preds = torch.argmax(outputs, dim=1)  # Convert logits to class indices\n",
        "                miou = calculate_iou(preds, labels, num_classes)\n",
        "                total_miou += miou\n",
        "\n",
        "                # Log metrics\n",
        "                pbar.set_postfix({'Avg Loss': total_loss / (pbar.n + 1), 'Avg mIoU': total_miou / (pbar.n + 1)})\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    avg_miou = total_miou / len(dataloader)\n",
        "    return avg_loss, avg_miou\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W37aSie9l6vq"
      },
      "source": [
        "#Implement Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8OKQuDCUl9MB"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, delta=0, save_path='best_model.pth'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How many epochs to wait after last improvement.\n",
        "            delta (float): Minimum change to qualify as an improvement.\n",
        "            save_path (str): Path to save the best model.\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "        self.save_path = save_path\n",
        "        self.best_score = None\n",
        "        self.epochs_without_improvement = 0\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        # If it's the first epoch, save the initial model\n",
        "        if self.best_score is None:\n",
        "            self.best_score = val_loss\n",
        "            self.save_checkpoint(model)\n",
        "        # Check if the current validation loss is better than the best score recorded\n",
        "        elif val_loss < self.best_score - self.delta:\n",
        "            self.best_score = val_loss\n",
        "            self.epochs_without_improvement = 0\n",
        "            self.save_checkpoint(model)\n",
        "        else:\n",
        "            self.epochs_without_improvement += 1\n",
        "            if self.epochs_without_improvement >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "    def save_checkpoint(self, model):\n",
        "        \"\"\"Save the model when validation loss decreases.\"\"\"\n",
        "        torch.save(model.state_dict(), self.save_path)\n",
        "        print(f\"Model improved\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwHerGifk-Fi"
      },
      "source": [
        "#Train model and use loss,MIoU and validation loss to make improvements on overfitting or underfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PCcTOmxjXwpe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "f62497ea-350a-4870-ae52-6c40dc1328e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-e8c929a9e886>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Train the model for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-2fd936c15c1b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, criterion, optimizer, device, num_classes)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# Backward pass and optimizer step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "# Reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = SemanticSegmentationModel().to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "miou_scores = []\n",
        "\n",
        "# Initialize early stopping\n",
        "early_stopping = EarlyStopping(patience=5, delta=0.001, save_path='best_model.pth')\n",
        "\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    # Train the model for one epoch\n",
        "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Validate the model\n",
        "    val_loss, val_miou = validate(model, val_loader, criterion, device)\n",
        "    val_losses.append(val_loss)\n",
        "    miou_scores.append(val_miou)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | mIoU: {val_miou:.4f}\")\n",
        "\n",
        "    # Check early stopping condition\n",
        "    early_stopping(val_loss, model)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered. Stopping training.\")\n",
        "        break\n",
        "\n",
        "# Save final metrics for analysis\n",
        "final_metrics = {\n",
        "    \"train_losses\": train_losses,\n",
        "    \"val_losses\": val_losses,\n",
        "    \"miou_scores\": miou_scores\n",
        "}\n",
        "torch.save(final_metrics, 'training_metrics.pth')\n",
        "\n",
        "print(\"Training complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahiXv1y-T3IN"
      },
      "source": [
        "#Vizualize the model's predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7OPyAR9T5lP"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "def visualize_batch(images, labels, preds, class_colors, class_names=None):\n",
        "    \"\"\"\n",
        "    Visualizes a batch of original images, target masks, and predicted masks.\n",
        "\n",
        "    Args:\n",
        "        images: Tensor of shape (batch_size, channels, height, width)\n",
        "        labels: Tensor of shape (batch_size, height, width)\n",
        "        preds: Tensor of shape (batch_size, height, width)\n",
        "        class_colors: List of RGB tuples for each class.\n",
        "        class_names: Optional list of class names for a legend.\n",
        "    \"\"\"\n",
        "    batch_size = images.shape[0]\n",
        "\n",
        "    # Create a color map for segmentation masks\n",
        "    cmap = ListedColormap(np.array(class_colors) / 255.0)  # Normalize colors to [0, 1]\n",
        "\n",
        "    # Create a grid of subplots: each row for one sample (original, ground truth, prediction)\n",
        "    fig, axes = plt.subplots(batch_size, 3, figsize=(15, 5 * batch_size))\n",
        "\n",
        "    # If batch size is 1, adjust axes to avoid indexing issues\n",
        "    if batch_size == 1:\n",
        "        axes = axes[np.newaxis, :]\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        # Convert image to numpy and normalize to [0, 1]\n",
        "        img = images[i].permute(1, 2, 0).cpu().numpy()  # (height, width, channels)\n",
        "        img = (img - img.min()) / (img.max() - img.min())  # Normalize for visualization\n",
        "\n",
        "        # Convert labels and predictions to numpy arrays\n",
        "        label = labels[i].cpu().numpy()\n",
        "        pred = preds[i].cpu().numpy()\n",
        "\n",
        "        # Plot the original image\n",
        "        axes[i, 0].imshow(img)\n",
        "        axes[i, 0].set_title(\"Original Image\")\n",
        "        axes[i, 0].axis(\"off\")\n",
        "\n",
        "        # Plot the ground truth with the colormap\n",
        "        axes[i, 1].imshow(label, cmap=cmap, interpolation=\"nearest\")\n",
        "        axes[i, 1].set_title(\"Ground Truth\")\n",
        "        axes[i, 1].axis(\"off\")\n",
        "\n",
        "        # Plot the predictions with the colormap\n",
        "        axes[i, 2].imshow(pred, cmap=cmap, interpolation=\"nearest\")\n",
        "        axes[i, 2].set_title(\"Prediction\")\n",
        "        axes[i, 2].axis(\"off\")\n",
        "\n",
        "    # Add a legend for class names if provided\n",
        "    if class_names is not None:\n",
        "        from matplotlib.patches import Patch\n",
        "        legend_elements = [\n",
        "            Patch(facecolor=np.array(color) / 255.0, label=name)\n",
        "            for color, name in zip(class_colors, class_names)\n",
        "        ]\n",
        "        fig.legend(\n",
        "            handles=legend_elements,\n",
        "            loc=\"upper center\",\n",
        "            ncol=len(class_names),\n",
        "            bbox_to_anchor=(0.5, 0.95),\n",
        "        )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Example Usage:\n",
        "# Assuming you have a DataLoader (`val_loader`) and a trained model\n",
        "for images, labels in val_loader:\n",
        "    # Move images and labels to the device (e.g., GPU)\n",
        "    images = images.to(device)  # Shape: (batch_size, channels, height, width)\n",
        "    labels = labels.to(device)  # Shape: (batch_size, height, width)\n",
        "\n",
        "    # Get predictions\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images)[0]  # Outputs shape: (batch_size, num_classes, height, width)\n",
        "        preds = torch.argmax(outputs, dim=1)  # Convert logits to class indices\n",
        "\n",
        "    # Define class colors (RGB tuples)\n",
        "    class_colors = [\n",
        "    (0, 0, 0),         # void: Black\n",
        "    (128, 64, 128),    # flat (road, sidewalk): Purple\n",
        "    (70, 70, 70),      # construction (building, wall): Dark gray\n",
        "    (153, 153, 153),   # object (poles, signs): Light gray\n",
        "    (107, 142, 35),    # nature (trees, vegetation): Green\n",
        "    (70, 130, 180),    # sky: Light blue\n",
        "    (220, 20, 60),     # human (pedestrians, cyclists): Red\n",
        "    (0, 0, 142)        # vehicle: Dark blue\n",
        "]\n",
        "\n",
        "    # Define class names\n",
        "    class_names = [\n",
        "        \"void\",\n",
        "        \"flat\",\n",
        "        \"construction\",\n",
        "        \"object\",\n",
        "        \"nature\",\n",
        "        \"sky\",\n",
        "        \"human\",\n",
        "        \"vehicle\",\n",
        "    ]\n",
        "\n",
        "    # Visualize the batch\n",
        "    visualize_batch(images, labels, preds, class_colors, class_names)\n",
        "    break  # Exit after visualizing the first batch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y529P9GdpJDe"
      },
      "source": [
        "#Vizualize the convergance of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HWDtjPppMVv"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot Training and Validation Loss\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss vs. Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot mIoU vs. Epochs\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(miou_scores, label='Validation mIoU', color='orange')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('mIoU')\n",
        "plt.title('mIoU vs. Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "CD2v9XmZtm5g",
        "m9XikfJxkp5z",
        "a-1U2IMmkwYw",
        "jpRLHCtck0ac",
        "W37aSie9l6vq"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}